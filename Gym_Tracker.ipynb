{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOonvBoxqyLmy6/1IVf6P1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yogeshkumar170201/Yogeshkumar170201/blob/master/Gym_Tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle"
      ],
      "metadata": {
        "id": "xyXwmhzNNOAw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "P4kbEczsNSVV",
        "outputId": "019a08f0-9bef-4d49-8595-a149deccf629"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-34df7071-3ca6-43bb-978e-60f74cb815c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-34df7071-3ca6-43bb-978e-60f74cb815c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ykdahiya\",\"key\":\"d38b7d9f508bcc40efcc891626f72ca3\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "DZeFNCNgOKel"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "SA1WsvtiON4o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "weoheKFMOQc1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lpw4VodOWM_",
        "outputId": "ffd10838-9003-4d28-975c-c1b074dbfd15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                        title                                        size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------  ------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "themrityunjaypathak/covid-cases-and-deaths-worldwide       Covid Cases and Deaths WorldWide              8KB  2023-02-01 12:22:51          10268        342  1.0              \n",
            "datascientistanna/customers-dataset                        Shop Customer Data                           23KB  2023-02-07 18:42:21           8667        204  1.0              \n",
            "justin2028/unemployment-in-america-per-us-state            Unemployment in America, Per US State       826KB  2023-03-02 07:26:03            821         45  1.0              \n",
            "anas123siddiqui/mobiles                                    Mobiles                                      59KB  2023-02-18 16:37:24           1433         43  1.0              \n",
            "amaanansari09/top-100-songs                                Top 100 songs                                 6KB  2023-02-16 18:55:35           2729         83  1.0              \n",
            "thedevastator/airbnb-prices-in-european-cities             Airbnb Prices in European Cities              4MB  2023-02-20 09:48:04           2930         60  1.0              \n",
            "rkiattisak/salaly-prediction-for-beginer                   Salary prediction for beginner                3KB  2023-03-07 02:45:11            437         27  1.0              \n",
            "nikhilmahajan29/crop-production-statistics-india           Crop Production Statistics - India            3MB  2023-02-28 18:45:13            955         31  1.0              \n",
            "rajugc/imdb-top-250-movies-dataset                         IMDB Top 250 Movies Dataset                  52KB  2023-02-11 16:02:01           3613         82  1.0              \n",
            "karkavelrajaj/amazon-sales-dataset                         Amazon Sales Dataset                          2MB  2023-01-17 06:21:15          10920        163  1.0              \n",
            "ulrikthygepedersen/mushroom-attributes                     Mushroom Attributes                          58KB  2023-02-10 08:58:02            889         29  1.0              \n",
            "rajkumarpandey02/countries-by-carbon-dioxide-emissions     Countries by Carbon Dioxide Emissions.        7KB  2023-02-25 17:51:36            942         36  1.0              \n",
            "shrikrishnaparab/forbes-billionaires-and-companies-2022    Forbes Billionaires and Companies 2022      735KB  2023-02-17 06:15:43           1033         24  1.0              \n",
            "niraliivaghani/chatbot-dataset                             Chatbot dataset                               5KB  2023-02-19 05:49:34           1041         35  1.0              \n",
            "thedevastator/bigmart-product-sales-factors                BigMart Product Sales Factors               367KB  2023-02-10 10:36:55            957         28  0.88235295       \n",
            "warcoder/earthquake-dataset                                Earthquake dataset                           35KB  2023-02-26 09:59:04            909         33  1.0              \n",
            "belayethossainds/the-worlds-billionaires-dataset-19872022  The World's Billionaires Dataset 1987~2022    5KB  2023-02-23 07:14:19            572         29  1.0              \n",
            "thedevastator/domestic-food-prices-after-covid-19          Domestic Food Prices After COVID-19           1MB  2023-02-13 01:45:15           2533         47  1.0              \n",
            "warcoder/dog-breeds-details                                Dog breeds details                            3KB  2023-03-02 14:01:08            733         30  0.9411765        \n",
            "dansbecker/melbourne-housing-snapshot                      Melbourne Housing Snapshot                  451KB  2018-06-05 12:52:24         109158       1222  0.7058824        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hasyimabdillah/workoutfitness-video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ek1cSxJOemX",
        "outputId": "87f47d2f-d4df-4690-92cc-11331463ad0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading workoutfitness-video.zip to /content\n",
            "100% 2.99G/2.99G [03:03<00:00, 16.9MB/s]\n",
            "100% 2.99G/2.99G [03:03<00:00, 17.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNNEnt2lOvxL",
        "outputId": "02dae15c-d87c-4929-f71e-03266f6e22d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-25ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/workoutfitness-video.zip -d /content/datset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naMtg-ROOyV6",
        "outputId": "63e0b9a4-d40a-4c54-b521-0ec9bded9746"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/workoutfitness-video.zip\n",
            "  inflating: /content/datset/Bench press/bench press_1.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_10.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_11.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_12.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_13.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_14.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_15.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_16.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_17.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_18.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_19.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_2.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_20.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_21.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_22.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_23.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_24.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_25.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_26.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_27.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_28.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_3.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_4.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_5.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_6.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_7.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_8.mp4  \n",
            "  inflating: /content/datset/Bench press/bench press_9.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_1.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_10.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_11.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_12.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_13.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_14.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_15.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_16.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_17.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_18.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_19.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_2.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_20.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_21.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_22.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_23.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_24.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_25.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_26.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_27.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_28.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_29.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_3.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_30.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_31.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_32.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_33.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_34.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_35.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_36.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_37.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_38.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_39.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_4.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_5.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_6.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_7.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_8.mp4  \n",
            "  inflating: /content/datset/Biceps curl/biceps curl_9.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_1.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_10.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_11.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_12.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_13.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_14.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_15.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_16.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_17.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_18.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_19.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_2.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_20.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_21.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_3.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_4.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_5.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_6.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_7.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_8.mp4  \n",
            "  inflating: /content/datset/Chest fly machine/chest fly machine_9.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_1.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_10.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_11.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_12.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_13.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_14.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_15.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_16.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_17.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_18.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_19.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_2.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_20.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_21.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_22.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_23.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_24.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_3.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_4.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_5.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_6.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_7.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_8.mp4  \n",
            "  inflating: /content/datset/Deadlift/deadlift_9.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_1.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_10.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_11.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_12.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_13.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_14.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_15.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_16.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_17.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_18.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_19.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_2.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_20.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_21.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_22.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_23.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_24.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_25.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_3.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_4.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_5.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_6.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_7.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_8.mp4  \n",
            "  inflating: /content/datset/Incline bench press/ibc_9.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_1.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_10.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_11.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_12.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_13.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_14.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_15.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_16.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_17.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_18.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_19.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_2.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_20.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_21.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_22.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_23.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_24.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_25.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_3.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_4.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_5.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_6.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_7.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_8.mp4  \n",
            "  inflating: /content/datset/Lat pulldown/lat pulldown_9.mp4  \n",
            "  inflating: /content/datset/Pull Up/pull up_1.MOV  \n",
            "  inflating: /content/datset/Pull Up/pull up_2.MOV  \n",
            "  inflating: /content/datset/Pull Up/pull up_3.MOV  \n",
            "  inflating: /content/datset/Pull Up/pull up_4.MOV  \n",
            "  inflating: /content/datset/Push-up/push-up_1.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_10.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_11.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_12.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_13.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_14.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_15.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_16.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_17.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_18.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_19.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_2.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_20.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_21.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_22.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_23.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_24.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_25.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_26.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_27.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_28.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_29.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_3.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_30.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_31.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_32.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_33.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_34.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_35.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_36.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_37.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_38.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_39.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_4.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_40.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_41.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_42.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_43.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_44.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_45.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_46.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_47.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_48.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_49.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_5.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_50.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_51.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_52.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_53.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_54.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_55.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_56.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_6.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_7.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_8.mp4  \n",
            "  inflating: /content/datset/Push-up/push-up_9.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_1.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_10.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_11.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_12.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_13.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_14.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_15.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_16.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_17.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_18.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_19.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_2.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_20.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_21.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_22.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_23.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_24.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_25.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_26.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_27.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_28.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_29.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_3.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_30.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_31.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_32.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_33.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_34.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_35.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_36.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_37.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_4.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_5.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_6.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_7.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_8.mp4  \n",
            "  inflating: /content/datset/Tricep Pushdown/tricep pushdown_9.mp4  \n",
            "  inflating: /content/datset/decline bench press/dbp_1.mp4  \n",
            "  inflating: /content/datset/decline bench press/dbp_2.MOV  \n",
            "  inflating: /content/datset/decline bench press/dbp_3.MOV  \n",
            "  inflating: /content/datset/decline bench press/dbp_4.MOV  \n",
            "  inflating: /content/datset/decline bench press/dbp_5.MOV  \n",
            "  inflating: /content/datset/decline bench press/dbp_6.MOV  \n",
            "  inflating: /content/datset/decline bench press/dbp_7.MOV  \n",
            "  inflating: /content/datset/decline bench press/dbp_8.mp4  \n",
            "  inflating: /content/datset/decline bench press/dbp_9.mp4  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_1.MOV  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_2.MOV  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_3.MOV  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_4.MOV  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_5.MOV  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_6.MOV  \n",
            "  inflating: /content/datset/hammer curl/hammer curl_7.MOV  \n",
            "  inflating: /content/datset/hip thrust/hip thrust_1.MOV  \n",
            "  inflating: /content/datset/hip thrust/hip thrust_2.MOV  \n",
            "  inflating: /content/datset/hip thrust/hip thrust_3.MOV  \n",
            "  inflating: /content/datset/hip thrust/hip thrust_4.MOV  \n",
            "  inflating: /content/datset/hip thrust/hip thrust_5.mp4  \n",
            "  inflating: /content/datset/lateral raise/lateral raise_1.MOV  \n",
            "  inflating: /content/datset/lateral raise/lateral raise_2.MOV  \n",
            "  inflating: /content/datset/lateral raise/lateral raise_3.MOV  \n",
            "  inflating: /content/datset/lateral raise/lateral raise_4.MOV  \n",
            "  inflating: /content/datset/lateral raise/lateral raise_5.MOV  \n",
            "  inflating: /content/datset/lateral raise/lateral raise_6.MOV  \n",
            "  inflating: /content/datset/leg extension/leg extension_1.MOV  \n",
            "  inflating: /content/datset/leg extension/leg extension_2.MOV  \n",
            "  inflating: /content/datset/leg extension/leg extension_3.MOV  \n",
            "  inflating: /content/datset/leg extension/leg extension_4.MOV  \n",
            "  inflating: /content/datset/leg extension/leg extension_5.MOV  \n",
            "  inflating: /content/datset/leg extension/leg extension_6.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_1.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_10.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_11.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_12.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_13.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_14.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_15.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_16.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_17.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_2.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_3.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_4.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_5.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_6.MOV  \n",
            "  inflating: /content/datset/leg raises/leg raises_7.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_8.mp4  \n",
            "  inflating: /content/datset/leg raises/leg raises_9.mp4  \n",
            "  inflating: /content/datset/plank/plank_1.MOV  \n",
            "  inflating: /content/datset/plank/plank_2.mp4  \n",
            "  inflating: /content/datset/plank/plank_3.mp4  \n",
            "  inflating: /content/datset/plank/plank_4.mp4  \n",
            "  inflating: /content/datset/plank/plank_5.mp4  \n",
            "  inflating: /content/datset/plank/plank_6.mp4  \n",
            "  inflating: /content/datset/plank/plank_7.mp4  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_1.mov  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_2.MOV  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_3.mov  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_4.MOV  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_5.mp4  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_6.mp4  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_7.mp4  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_8.mp4  \n",
            "  inflating: /content/datset/romanian deadlift/romanian deadlift_9.mp4  \n",
            "  inflating: /content/datset/russian twist/russian twist_1.MOV  \n",
            "  inflating: /content/datset/russian twist/russian twist_2.mp4  \n",
            "  inflating: /content/datset/russian twist/russian twist_3.mp4  \n",
            "  inflating: /content/datset/russian twist/russian twist_4.mp4  \n",
            "  inflating: /content/datset/russian twist/russian twist_5.mp4  \n",
            "  inflating: /content/datset/shoulder press/shoulder press_1.MOV  \n",
            "  inflating: /content/datset/shoulder press/shoulder press_2.MOV  \n",
            "  inflating: /content/datset/shoulder press/shoulder press_3.MOV  \n",
            "  inflating: /content/datset/shoulder press/shoulder press_4.MOV  \n",
            "  inflating: /content/datset/squat/squat_1.MOV  \n",
            "  inflating: /content/datset/squat/squat_2.MOV  \n",
            "  inflating: /content/datset/squat/squat_3.MOV  \n",
            "  inflating: /content/datset/squat/squat_4.MOV  \n",
            "  inflating: /content/datset/squat/squat_5.MOV  \n",
            "  inflating: /content/datset/squat/squat_6.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_1.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_2.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_3.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_4.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_5.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_6.MOV  \n",
            "  inflating: /content/datset/t bar row/t bar row_7.MOV  \n",
            "  inflating: /content/datset/tricep dips/tricep dips_1.MOV  \n",
            "  inflating: /content/datset/tricep dips/tricep dips_2.MOV  \n",
            "  inflating: /content/datset/tricep dips/tricep dips_3.MOV  \n",
            "  inflating: /content/datset/tricep dips/tricep dips_4.MOV  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "j1L0aMxGTKex"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the height and width to which each video frame will be resized in our dataset.\n",
        "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
        "\n",
        "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
        "SEQUENCE_LENGTH = 20\n",
        "\n",
        "# Specify the directory containing the UCF50 dataset. \n",
        "DATASET_DIR = \"datset\"\n",
        "\n",
        "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n",
        "CLASSES_LIST = os.listdir('/content/datset')"
      ],
      "metadata": {
        "id": "rqvUTkkHPFVl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_extraction(video_path):\n",
        "  \n",
        "    # Declare a list to store video frames.\n",
        "    frames_list = []\n",
        "    \n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total number of frames in the video.\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        "\n",
        "        # Set the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Reading the frame from the video. \n",
        "        success, frame = video_reader.read() \n",
        "\n",
        "        # Check if Video frame is not successfully read then break the loop\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed height and width.\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        \n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255\n",
        "        \n",
        "        # Append the normalized frame into the frames list\n",
        "        frames_list.append(normalized_frame)\n",
        "    \n",
        "    # Release the VideoCapture object. \n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return frames_list"
      ],
      "metadata": {
        "id": "A_-dfQYeS0r_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "\n",
        "    # Declared Empty Lists to store the features, labels and video file path values.\n",
        "    features = []\n",
        "    labels = []\n",
        "    video_files_paths = []\n",
        "    \n",
        "    # Iterating through all the classes mentioned in the classes list\n",
        "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "        print(class_index, class_name)\n",
        "        # Display the name of the class whose data is being extracted.\n",
        "        print(f'Extracting Data of Class: {class_name}')\n",
        "        \n",
        "        # Get the list of video files present in the specific class name directory.\n",
        "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
        "        \n",
        "        # Iterate through all the files present in the files list.\n",
        "        for file_name in files_list:\n",
        "            \n",
        "            # Get the complete video path.\n",
        "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
        "\n",
        "            # Extract the frames of the video file.\n",
        "            frames = frames_extraction(video_file_path)\n",
        "\n",
        "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
        "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
        "            if len(frames) == SEQUENCE_LENGTH:\n",
        "\n",
        "                # Append the data to their repective lists.\n",
        "                features.append(frames)\n",
        "                labels.append(class_index)\n",
        "                video_files_paths.append(video_file_path)\n",
        "\n",
        "    # Converting the list to numpy arrays\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)  \n",
        "    \n",
        "    # Return the frames, class index, and video file path.\n",
        "    return features, labels, video_files_paths"
      ],
      "metadata": {
        "id": "kHoFHW-PTnMA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset.\n",
        "features, labels, video_files_paths = create_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJkwWLP7Tpyw",
        "outputId": "0a2352f3-7cbd-4ca4-d668-85c041f600cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 leg extension\n",
            "Extracting Data of Class: leg extension\n",
            "1 romanian deadlift\n",
            "Extracting Data of Class: romanian deadlift\n",
            "2 Biceps curl\n",
            "Extracting Data of Class: Biceps curl\n",
            "3 decline bench press\n",
            "Extracting Data of Class: decline bench press\n",
            "4 Deadlift\n",
            "Extracting Data of Class: Deadlift\n",
            "5 leg raises\n",
            "Extracting Data of Class: leg raises\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n",
        "one_hot_encoded_labels = to_categorical(labels)"
      ],
      "metadata": {
        "id": "8wjv7YbGTuPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the Data into Train ( 75% ) and Test Set ( 25% ).\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)"
      ],
      "metadata": {
        "id": "tqPQJMNLTyAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_convlstm_model():\n",
        "    '''\n",
        "    This function will construct the required convlstm model.\n",
        "    Returns:\n",
        "        model: It is the required constructed convlstm model.\n",
        "    '''\n",
        "\n",
        "    # We will use a Sequential model for model construction\n",
        "    model = Sequential()\n",
        "\n",
        "    # Define the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
        "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    #model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(Flatten()) \n",
        "    \n",
        "    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n",
        "    \n",
        "    ########################################################################################################################\n",
        "     \n",
        "    # Display the models summary.\n",
        "    model.summary()\n",
        "    \n",
        "    # Return the constructed convlstm model.\n",
        "    return model"
      ],
      "metadata": {
        "id": "UdqrB0aBT1r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the required convlstm model.\n",
        "convlstm_model = create_convlstm_model()\n",
        "\n",
        "# Display the success message. \n",
        "print(\"Model Created Successfully!\")"
      ],
      "metadata": {
        "id": "UuJkBqUPT4pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the structure of the contructed model.\n",
        "plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "zb7o3I9IT62z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Instance of Early Stopping Callback\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "# Compile the model and specify loss function, optimizer and metrics values to the model\n",
        "convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        "\n",
        "# Start training the model.\n",
        "convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4,shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"
      ],
      "metadata": {
        "id": "H1FjYzqNT9hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model.\n",
        "model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)"
      ],
      "metadata": {
        "id": "eUjgHyHdUAOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the loss and accuracy from model_evaluation_history.\n",
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        "\n",
        "# Define the string date format.\n",
        "# Get the current Date and Time in a DateTime Object.\n",
        "# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        "\n",
        "# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n",
        "model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        "\n",
        "# Save your Model.\n",
        "convlstm_model.save(model_file_name)"
      ],
      "metadata": {
        "id": "XACiyS8RUDrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
        "    '''\n",
        "    This function will plot the metrics passed to it in a graph.\n",
        "    Args:\n",
        "        model_training_history: A history object containing a record of training and validation \n",
        "                                loss values and metrics values at successive epochs\n",
        "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
        "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
        "        plot_name:              The title of the graph.\n",
        "    '''\n",
        "    \n",
        "    # Get metric values using metric names as identifiers.\n",
        "    metric_value_1 = model_training_history.history[metric_name_1]\n",
        "    metric_value_2 = model_training_history.history[metric_name_2]\n",
        "    \n",
        "    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n",
        "    epochs = range(len(metric_value_1))\n",
        "\n",
        "    # Plot the Graph.\n",
        "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
        "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
        "\n",
        "    # Add title to the plot.\n",
        "    plt.title(str(plot_name))\n",
        "\n",
        "    # Add legend to the plot.\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "GI9Vh6djUGcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training and validation loss metrices.\n",
        "plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
      ],
      "metadata": {
        "id": "c-FU6weWUIb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training and validation accuracy metrices.\n",
        "plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy') "
      ],
      "metadata": {
        "id": "n7tuM99LUKmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}